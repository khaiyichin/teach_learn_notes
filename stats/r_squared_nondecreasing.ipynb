{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "r_squared_nondecreasing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Intuition behind the non-decreasing property of the coefficient of determination $R^2$\n",
        "\n",
        "To characterize the goodness of fit for regression models, the coefficient of determination, $R^2$, is commonly used. In unconstrained and reasonable models, $R^2$ ranges between 0 and 1 for a poor and a good fit respectively; the value provides the fraction of variance explainable by the model.\n",
        "\n",
        "Specifically, the formula for the coefficient of determination is given by:\n",
        "\n",
        "\\begin{align*}\n",
        "    R^2 &= 1 - \\frac{\\text{residual sum of squares}}{\\text{total sum of squares}} \\\\\n",
        "    &= 1 - \\frac{ \\sum_i{ (\\hat{y}_i - y_i)^2 } }{ \\sum_i{ (\\hat{y}_i - \\bar{y})^2 } } \\\\\n",
        "    &= 1 - \\frac{ \\sum_i{ {e_i}^2 } }{ \\sum_i{ (\\hat{y}_i - \\bar{y})^2 } }\n",
        "\\end{align*}\n",
        "\n",
        "where $y$ is the true output, $\\hat{y}$ is the predicted output, and $\\bar{y}$ is the mean of the true output. It follows that $e$ is the residual.\n",
        "\n",
        "One caveat in using $R^2$ is its susceptibility in reporting increasingly better fits for a model with an increasing number of features (model inputs). That is, **the more features we add to our model, the $R^2$ value will be at least the same as before (the addition of features), if not better**. This is caused by the weakly increasing property of the residual sum of squares, which can be explained as follows.\n",
        "\n",
        "Suppose that we are doing linear regression:\n",
        "\n",
        "\\begin{align*}\n",
        "    \\hat{y} &= X\\beta \\\\\n",
        "    \\hat{y} &\\in \\mathbb{R}^n, \\ X \\in \\mathbb{R}^{n \\times m}, \\ \\beta \\in \\mathbb{R}^m\n",
        "\\end{align*}\n",
        "\n",
        "which solves the following minimization problem, i.e., least-squares fit,\n",
        "\n",
        "\\begin{align*}\n",
        "    \\min_{\\beta} \\sum_i^n{ (\\hat{y} - y_i)^2 } &= \\min_{\\beta} \\sum_i^n{ (x_i \\beta - y_i)^2 } \\\\\n",
        "    &= \\min_{\\beta} \\sum_i^n{ [ (\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_m x_{im}) - y_i]^2 }.\n",
        "\\end{align*}\n",
        "\n",
        "Let's simplify by considering $n = 2$ and $m = 1$, which gives us \n",
        "\n",
        "\\begin{align*}\n",
        "    \\min_{\\beta} \\sum_i^2{ (x_i \\beta - y_i)^2 } &= \\min_{\\beta} [ (\\beta_0 + \\beta_1 x_{11} - y_1)^2 + (\\beta_0 + \\beta_1 x_{21} - y_2)^2 ]\n",
        "\\end{align*}\n",
        "\n",
        "and suppose that we find the optimal values $\\beta_0 = a$ and $\\beta_1 = b$ that turns the 1st term to 0 and the 2nd term to $\\epsilon$ to give us the score of $\\epsilon$:\n",
        "\n",
        "\\begin{align*}\n",
        "    \\min_{\\beta} \\sum_i^2{ (x_i \\beta - y_i)^2 } &= \\min_{\\beta} [ (\\beta_0 + \\beta_1 x_{11} - y_1)^2 + (\\beta_0 + \\beta_1 x_{21} - y_2)^2 ] \\\\\n",
        "    &= (a + b x_{11} - y_1)^2 + (a + b x_{21} - y_2)^2 \\\\\n",
        "    &= 0 + \\epsilon \\\\\n",
        "    &= \\epsilon.\n",
        "\\end{align*}\n",
        "\n",
        "As the optimization problem is the minimization of the residual sum of squares, $R^2$ is simply $1 - \\dfrac{\\epsilon}{\\sum_i^n{ (\\hat{y}_i - \\bar{y})^2 }} $.\n",
        "\n",
        "Now, if we add an addional feature $x_{i2}$ to be considered in our model, $m$ increases to 2 and the minimization problem becomes:\n",
        "\n",
        "\\begin{align*}\n",
        "    \\min_{\\beta} \\sum_i^2{ (\\hat{y} - y_i)^2 } &= \\min_{\\beta} [ (\\beta_0 + \\beta_1 x_{11} + \\beta_2 x_{12} - y_1)^2 + (\\beta_0 + \\beta_1 x_{21} + \\beta_2 x_{22} - y_2)^2 ].\n",
        "\\end{align*}\n",
        "\n",
        "This is where we see the non-decreasing property of $R^2$: if we use the same parameters as before ($\\beta_0 = a$ and $\\beta_1 = b$), then we are left with the residual $(\\beta_2 x_{12})^2 + (\\beta_2 x_{22} + \\epsilon)^2$.\n",
        "\n",
        "1. If we manage to find a $\\beta_2 = c$:\n",
        "\\begin{align*}\n",
        "    (\\beta_2 x_{12})^2 + (\\beta_2 x_{22} + \\sqrt{\\epsilon})^2 &= (c x_{12})^2 + (c x_{22}+ \\sqrt{\\epsilon})^2 \\\\\n",
        "    &= \\gamma\n",
        "\\end{align*}\n",
        "such that $\\gamma < \\epsilon$, then $R^2$ is now improved (higher than before the feature addition).\n",
        "\n",
        "2. Otherwise, if we aren't able to find a $\\beta_2 = c$ that reduces the residual, i.e., $\\gamma > \\epsilon$, then by definition the solution would be to set $\\beta_2 = c = 0$ to preserve the lowest possible score, which is simply $\\epsilon$.\n",
        "\n",
        "Therefore, $R^2$ is non-decreasing as the number of features increases &mdash; we say that $R^2$ weakly increases with the number of features (it either stays the same or increases).\n",
        "\n",
        "Reference: https://en.wikipedia.org/wiki/Coefficient_of_determination#Inflation_of_R2\n"
      ],
      "metadata": {
        "id": "HT9YjENYekAE"
      }
    }
  ]
}